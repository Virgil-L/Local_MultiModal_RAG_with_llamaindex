{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Qdrant Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy.vector_stores import QdrantVectorStore\n",
    "from custom_vectore_store import MultiModalQdrantVectorStore\n",
    "from custom_embeddings import custom_sparse_doc_vectors, custom_sparse_query_vectors\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qd_models\n",
    "\n",
    "try:\n",
    "    client = QdrantClient(path=\"qdrant_db\")\n",
    "    print(\"Connected to Qdrant\")\n",
    "except:\n",
    "    pass\n",
    "    print(\"Failed to connect to Qdrant\")\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "SPLADE_QUERY_PATH = \"./embedding_models/efficient-splade-VI-BT-large-query\"\n",
    "splade_q_tokenizer = AutoTokenizer.from_pretrained(SPLADE_QUERY_PATH)\n",
    "splade_q_model = AutoModelForMaskedLM.from_pretrained(SPLADE_QUERY_PATH)\n",
    "\n",
    "SPLADE_DOC_PATH = \"./embedding_models/efficient-splade-VI-BT-large-doc\"\n",
    "splade_d_tokenizer = AutoTokenizer.from_pretrained(SPLADE_DOC_PATH)\n",
    "splade_d_model = AutoModelForMaskedLM.from_pretrained(SPLADE_DOC_PATH)\n",
    "\n",
    "custom_sparse_doc_fn = partial(custom_sparse_doc_vectors, splade_d_tokenizer, splade_d_model, 512)\n",
    "custom_sparse_query_fn = partial(custom_sparse_query_vectors, splade_q_tokenizer, splade_q_model, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"text_collection\",\n",
    "    enable_hybrid=True,\n",
    "    sparse_query_fn=custom_sparse_query_fn,\n",
    "    sparse_doc_fn=custom_sparse_doc_fn,\n",
    "    stores_text=True,\n",
    ")\n",
    "\n",
    "image_store = MultiModalQdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"image_collection\",\n",
    "    enable_hybrid=True,\n",
    "    sparse_query_fn=custom_sparse_query_fn,\n",
    "    sparse_doc_fn=custom_sparse_doc_fn,\n",
    "    stores_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy.embeddings import HuggingFaceEmbedding\n",
    "from custom_embeddings import CustomizedCLIPEmbedding\n",
    "\n",
    "BGE_PATH = \"./embedding_models/bge-small-en-v1.5\"\n",
    "CLIP_PATH = \"./embedding_models/clip-vit-base-patch32\"\n",
    "bge_embedding = HuggingFaceEmbedding(model_name=BGE_PATH, device=\"cpu\", pooling=\"mean\")\n",
    "clip_embedding = CustomizedCLIPEmbedding(model_name=CLIP_PATH, device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Multi-modal Retriever with Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "bge_reranker = SentenceTransformerRerank(\n",
    "    model=\"./embedding_models/bge-reranker-base\",\n",
    "    top_n=3,\n",
    "    device=\"cpu\",\n",
    "    keep_retrieval_score=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mm_retriever import MultiModalQdrantRetriever\n",
    "\n",
    "mm_retriever = MultiModalQdrantRetriever(\n",
    "    text_vector_store = text_store,\n",
    "    image_vector_store = image_store, \n",
    "    text_embed_model = bge_embedding, \n",
    "    mm_embed_model = clip_embedding,\n",
    "    reranker = bge_reranker,\n",
    "    text_similarity_top_k = 5,\n",
    "    text_sparse_top_k = 5,\n",
    "    text_rerank_top_n = 3,\n",
    "    image_similarity_top_k = 5,\n",
    "    image_sparse_top_k = 5,\n",
    "    image_rerank_top_n = 1,\n",
    "    sparse_query_fn = custom_sparse_query_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy.schema import QueryBundle\n",
    "query_bundle=QueryBundle(query_str=\"How does Llama 2 perform compared to other open-source models?\")\n",
    "\n",
    "# text_query_result = mm_retriever.retrieve_text_nodes(query_bundle=query_bundle, query_mode=\"hybrid\")\n",
    "# reranked_text_nodes = mm_retriever.rerank_text_nodes(query_bundle, text_query_result)\n",
    "# image_query_result = mm_retriever.retrieve_image_nodes(query_bundle=query_bundle, query_mode=\"hybrid\")\n",
    "# reranked_image_nodes = mm_retriever.rerank_image_nodes(query_bundle, image_query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Quantized LLaVA-1.6 with llama-cpp framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp.llama_chat_format import Llava15ChatHandler\n",
    "\n",
    "llava_chat_handler = Llava15ChatHandler(\n",
    "    clip_model_path = \"LLMs/llava-1.6-mistral-7b-gguf/mmproj-model-f16.gguf\",\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "\n",
    "## Load LLaVA with the original llama-cpp python bindings \n",
    "\n",
    "# from llama_cpp import Llama\n",
    "\n",
    "# llava_1_6 = Llama(\n",
    "#     model_path=\"LLMs/llava-1.6-mistral-7b-gguf/llava-v1.6-mistral-7b.Q4_K_M.gguf\",\n",
    "#     chat_format=\"llava-1-5\",\n",
    "#     chat_handler=llava_chat_handler, # Optional chat handler to use when calling create_chat_completion.\n",
    "#     n_ctx=2048, # (context window size) Text context, 0 = from model\n",
    "#     logits_all=True, # Return logits for all tokens, not just the last token. Must be True for completion to return logprobs.\n",
    "#     offload_kqv=True, # Offload K, Q, V to GPU.\n",
    "#     n_gpu_layers=40,  # Number of layers to offload to GPU (-ngl). If -1, all layers are offloaded.\n",
    "#     last_n_tokens_size=64, # maximum number of tokens to keep in the last_n_tokens deque.\n",
    "#     verbose=True,\n",
    "\n",
    "#     ## LoRA Params\n",
    "#     # lora_base: Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.\n",
    "#     # lora_scale: float = 1.0,\n",
    "#     # lora_path: Path to a LoRA file to apply to the model.\n",
    "\n",
    "#     ## Tokenizer Override\n",
    "#     # tokenizer: Optional[BaseLlamaTokenizer] = None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load LLaVA with customized llama-index integration\n",
    "from llava_llamacpp import Llava_LlamaCPP\n",
    "\n",
    "model_kwargs = {\n",
    "    \"chat_format\":\"llava-1-5\",\n",
    "    \"chat_handler\":llava_chat_handler, \n",
    "    \"logits_all\":True,\n",
    "    \"offload_kqv\":True,\n",
    "    \"n_gpu_layers\":40,\n",
    "    \"last_n_tokens_size\":64,\n",
    "    \n",
    "    ## LoRA Params\n",
    "    # lora_base: Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.\n",
    "    # lora_scale: float = 1.0,\n",
    "    # lora_path: Path to a LoRA file to apply to the model.\n",
    "\n",
    "    ## Tokenizer Override\n",
    "    # tokenizer: Optional[BaseLlamaTokenizer] = None,\n",
    "}\n",
    "\n",
    "llava_1_6 = Llava_LlamaCPP(\n",
    "    model_path=\"LLMs/llava-1.6-mistral-7b-gguf/llava-v1.6-mistral-7b.Q3_K_M.gguf\",\n",
    "    temperature=0.5,\n",
    "    max_new_tokens=1024,\n",
    "    context_window=4096,\n",
    "    verbose=True,\n",
    "    model_kwargs = model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mm_query_engine import CustomMultiModalQueryEngine\n",
    "\n",
    "query_engine = CustomMultiModalQueryEngine(\n",
    "    retriever = mm_retriever,\n",
    "    multi_modal_llm = llava_1_6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval_results = query_engine.retrieve(query_bundle=query_bundle, text_query_mode=\"hybrid\", image_query_mode=\"default\")\n",
    "# response = query_engine.synthesize(query_bundle, retrieval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(query_bundle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu118py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
