{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Qdrant Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy.vector_stores import QdrantVectorStore\n",
    "from custom_vectore_store import MultiModalQdrantVectorStore\n",
    "from custom_embeddings import custom_sparse_doc_vectors, custom_sparse_query_vectors\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qd_models\n",
    "\n",
    "try:\n",
    "    client = QdrantClient(path=\"qdrant_db\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# client = QdrantClient(path=\"qdrant_db\")\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "SPLADE_QUERY_PATH = \"./embedding_models/efficient-splade-VI-BT-large-query\"\n",
    "splade_q_tokenizer = AutoTokenizer.from_pretrained(SPLADE_QUERY_PATH)\n",
    "splade_q_model = AutoModelForMaskedLM.from_pretrained(SPLADE_QUERY_PATH)\n",
    "\n",
    "SPLADE_DOC_PATH = \"./embedding_models/efficient-splade-VI-BT-large-doc\"\n",
    "splade_d_tokenizer = AutoTokenizer.from_pretrained(SPLADE_DOC_PATH)\n",
    "splade_d_model = AutoModelForMaskedLM.from_pretrained(SPLADE_DOC_PATH)\n",
    "\n",
    "custom_sparse_doc_fn = partial(custom_sparse_doc_vectors, splade_d_tokenizer, splade_d_model, 512)\n",
    "custom_sparse_query_fn = partial(custom_sparse_query_vectors, splade_q_tokenizer, splade_q_model, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"text_collection\",\n",
    "    enable_hybrid=True,\n",
    "    sparse_query_fn=custom_sparse_query_fn,\n",
    "    sparse_doc_fn=custom_sparse_doc_fn,\n",
    "    stores_text=True,\n",
    ")\n",
    "\n",
    "image_store = MultiModalQdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"image_collection\",\n",
    "    enable_hybrid=True,\n",
    "    sparse_query_fn=custom_sparse_query_fn,\n",
    "    sparse_doc_fn=custom_sparse_doc_fn,\n",
    "    stores_text=False,\n",
    ")\n",
    "\n",
    "    # def __init__(\n",
    "    #     self,\n",
    "    #     collection_name: str,\n",
    "    #     client: Optional[Any] = None,\n",
    "    #     aclient: Optional[Any] = None,\n",
    "    #     url: Optional[str] = None,\n",
    "    #     api_key: Optional[str] = None,\n",
    "    #     batch_size: int = 64,\n",
    "    #     parallel: int = 1,\n",
    "    #     max_retries: int = 3,\n",
    "    #     client_kwargs: Optional[dict] = None,\n",
    "    #     enable_hybrid: bool = False,\n",
    "    #     sparse_doc_fn: Optional[SparseEncoderCallable] = None,\n",
    "    #     sparse_query_fn: Optional[SparseEncoderCallable] = None,\n",
    "    #     hybrid_fusion_fn: Optional[HybridFusionCallable] = None,\n",
    "    #     **kwargs: Any,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy.embeddings import HuggingFaceEmbedding\n",
    "from custom_embeddings import CustomizedCLIPEmbedding\n",
    "\n",
    "BGE_PATH = \"./embedding_models/bge-small-en-v1.5\"\n",
    "CLIP_PATH = \"./embedding_models/clip-vit-base-patch32\"\n",
    "bge_embedding = HuggingFaceEmbedding(model_name=BGE_PATH, device=\"cpu\", pooling=\"mean\")\n",
    "clip_embedding = CustomizedCLIPEmbedding(model_name=CLIP_PATH, device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Multi-modal Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy.schema import QueryBundle\n",
    "from llama_index.legacy.retrievers import BaseRetriever\n",
    "from llama_index.legacy.schema import NodeWithScore\n",
    "from llama_index.legacy.vector_stores import VectorStoreQuery\n",
    "from llama_index.legacy.vector_stores.types import VectorStoreQueryMode\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "\n",
    "class MultiModalQdrantRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a qdrant vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_vector_store: QdrantVectorStore = None,\n",
    "        image_vector_store: MultiModalQdrantVectorStore = None,\n",
    "        \n",
    "        text_embed_model: Any = None,\n",
    "        mm_embed_model: Any = None,\n",
    "\n",
    "\n",
    "        text_similarity_top_k: int = 5,\n",
    "        text_sparse_top_k: int = 5,\n",
    "        image_similarity_top_k: int = 5,\n",
    "        image_sparse_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._text_vector_store = text_vector_store\n",
    "        self._image_vector_store = image_vector_store\n",
    "        self._text_embed_model = text_embed_model\n",
    "        self._mm_embed_model = mm_embed_model\n",
    "\n",
    "        self._text_similarity_top_k = text_similarity_top_k\n",
    "        self._text_sparse_top_k = text_sparse_top_k\n",
    "        self._image_similarity_top_k = image_similarity_top_k\n",
    "        self._image_sparse_top_k = image_sparse_top_k\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def retrieve_text_nodes(self, query_bundle: QueryBundle, query_mode: str=\"hybrid\", metadata_filters=None):\n",
    "\n",
    "        query_embedding = self._text_embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "\n",
    "        # query with dense text embedding\n",
    "        dense_query = VectorStoreQuery(\n",
    "            query_str=query_bundle.query_str,\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._text_similarity_top_k,\n",
    "            sparse_top_k=self._text_sparse_top_k,\n",
    "            mode=VectorStoreQueryMode.DEFAULT,\n",
    "            filters=metadata_filters,\n",
    "        )\n",
    "\n",
    "        # query with sparse text vector\n",
    "        sparse_query = VectorStoreQuery(\n",
    "            query_str=query_bundle.query_str,\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._text_similarity_top_k,\n",
    "            sparse_top_k=self._text_sparse_top_k,\n",
    "            mode=VectorStoreQueryMode.SPARSE,\n",
    "            filters=metadata_filters,\n",
    "        )\n",
    "\n",
    "        # mm_query = VectorStoreQuery(...)\n",
    "        \n",
    "        # returns a VectorStoreQueryResult\n",
    "        if query_mode == \"default\":\n",
    "            dense_query_result = self._text_vector_store.query(dense_query)\n",
    "            \n",
    "            return {\n",
    "                \"text-dense\": dense_query_result\n",
    "            }\n",
    "\n",
    "        elif query_mode == \"sparse\":\n",
    "            sparse_query_result = self._text_vector_store.query(sparse_query)\n",
    "            \n",
    "            return {\n",
    "                \"text-sparse\": sparse_query_result\n",
    "            }\n",
    "\n",
    "\n",
    "        elif query_mode == \"hybrid\":\n",
    "            dense_query_result = self._text_vector_store.query(dense_query)\n",
    "            sparse_query_result = self._text_vector_store.query(sparse_query)\n",
    "\n",
    "            return {\n",
    "                \"text-dense\": dense_query_result,\n",
    "                \"text-sparse\": sparse_query_result\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid text-to-text query mode: {query_mode}, must be one of ['default', 'sparse', 'hybrid']\")\n",
    "        \n",
    "\n",
    "\n",
    "    def retrieve_image_nodes(self, query_bundle: QueryBundle, query_mode: str=\"default\", metadata_filters=None):\n",
    "\n",
    "        \n",
    "        if query_mode == \"default\": # Default: query with dense multi-modal embedding only\n",
    "            mm_query = VectorStoreQuery(\n",
    "                query_str=query_bundle.query_str,\n",
    "                query_embedding=self._mm_embed_model.get_query_embedding(query_bundle.query_str),\n",
    "                similarity_top_k=self._image_similarity_top_k,\n",
    "                mode=VectorStoreQueryMode.DEFAULT,\n",
    "                filters=metadata_filters,\n",
    "            )\n",
    "            mm_query_result = self._image_vector_store.text_to_image_query(mm_query)\n",
    "            \n",
    "            return {\n",
    "                \"multi-modal\": mm_query_result\n",
    "            }\n",
    "\n",
    "\n",
    "        elif query_mode == \"text-dense\":\n",
    "            text_dense_query = VectorStoreQuery(\n",
    "                query_str=query_bundle.query_str,\n",
    "                query_embedding=self._text_embed_model.get_query_embedding(query_bundle.query_str),\n",
    "                similarity_top_k=self._image_similarity_top_k,\n",
    "                mode=VectorStoreQueryMode.DEFAULT,\n",
    "                filters=metadata_filters,\n",
    "            )\n",
    "            text_dense_query_result = self._image_vector_store.text_to_caption_query(text_dense_query)\n",
    "            \n",
    "            return {\n",
    "                \"text-dense\": text_dense_query_result\n",
    "            }\n",
    "\n",
    "\n",
    "        elif query_mode == \"text-sparse\":\n",
    "            text_sparse_query = VectorStoreQuery(\n",
    "                query_str=query_bundle.query_str,\n",
    "                #query_embedding=self._text_embed_model.get_query_embedding(query_bundle.query_str),\n",
    "                similarity_top_k=self._image_sparse_top_k,\n",
    "                mode=VectorStoreQueryMode.SPARSE,\n",
    "                filters=metadata_filters,\n",
    "            )\n",
    "            text_sparse_query_result = self._image_vector_store.text_to_caption_query(text_sparse_query)\n",
    "            \n",
    "            return {\n",
    "                \"text-sparse\": text_sparse_query_result\n",
    "            }\n",
    "\n",
    "        elif query_mode == \"hybrid\":\n",
    "            mm_query = VectorStoreQuery(\n",
    "                query_str=query_bundle.query_str,\n",
    "                query_embedding=self._mm_embed_model.get_query_embedding(query_bundle.query_str),\n",
    "                similarity_top_k=self._image_similarity_top_k,\n",
    "                mode=VectorStoreQueryMode.DEFAULT,\n",
    "                filters=metadata_filters,\n",
    "            )\n",
    "            mm_query_result = self._image_vector_store.text_to_image_query(mm_query)\n",
    "\n",
    "            text_dense_query = VectorStoreQuery(\n",
    "                query_str=query_bundle.query_str,\n",
    "                query_embedding=self._text_embed_model.get_query_embedding(query_bundle.query_str),\n",
    "                similarity_top_k=self._image_similarity_top_k,\n",
    "                mode=VectorStoreQueryMode.DEFAULT,\n",
    "                filters=metadata_filters,\n",
    "            )\n",
    "            text_dense_query_result = self._image_vector_store.text_to_caption_query(text_dense_query)\n",
    "            \n",
    "            text_sparse_query = VectorStoreQuery(\n",
    "                query_str=query_bundle.query_str,\n",
    "                #query_embedding=self._text_embed_model.get_query_embedding(query_bundle.query_str),\n",
    "                similarity_top_k=self._image_sparse_top_k,\n",
    "                mode=VectorStoreQueryMode.SPARSE,\n",
    "                filters=metadata_filters,\n",
    "            )\n",
    "            text_sparse_query_result = self._image_vector_store.text_to_caption_query(text_sparse_query)\n",
    "\n",
    "            return {\n",
    "                \"multi-modal\": mm_query_result,\n",
    "                \"text-dense\": text_dense_query_result,\n",
    "                \"text-sparse\": text_sparse_query_result\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid text-to-image query mode: {query_mode}, must be one of ['default', 'text-dense', 'text-sparse', 'hybrid']\")\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle, query_mode: str=\"hybrid\", metadata_filters=None):\n",
    "        ###TODO: fix bug in `self._handle_recursive_retrieval` from BaseRetriever\n",
    "        # return {\n",
    "        #     \"text_nodes\": self.retrieve_text_nodes(query_bundle, query_mode, metadata_filters),\n",
    "        #     \"image_nodes\": self.retrieve_image_nodes(query_bundle, query_mode, metadata_filters)\n",
    "        # }\n",
    "\n",
    "        \"\"\" Deprecated abstract retrieve method from the BaseRetriever\"\"\"\n",
    "\n",
    "        query_embedding = self._text_embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )        \n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._text_similarity_top_k,\n",
    "            mode=\"default\",\n",
    "        )\n",
    "\n",
    "        query_result = self._text_vector_store.query(vector_store_query)\n",
    "        \n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores\n",
    "\n",
    "    ### TODO: rewrite the following methods to use the new retrieve_text_nodes and retrieve_image_nodes\n",
    "\n",
    "    # def retrieve(self, str_or_query_bundle: QueryType):\n",
    "    #     return\n",
    "\n",
    "    # async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "    #     \"\"\"Asynchronously retrieve nodes given query.\n",
    "\n",
    "    #     Implemented by the user.\n",
    "\n",
    "    #     \"\"\"\n",
    "    #     return self._retrieve(query_bundle)\n",
    "\n",
    "    # async def aretrieve(self, str_or_query_bundle: QueryType):\n",
    "    #     return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_retriever = MultiModalQdrantRetriever(\n",
    "    text_vector_store = text_store,\n",
    "    image_vector_store = image_store, \n",
    "    text_embed_model = bge_embedding, \n",
    "    mm_embed_model = clip_embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query_result = mm_retriever.retrieve_text_nodes(query_bundle=QueryBundle(query_str=\"How does Llama 2 perform compared to other open-source models?\"), query_mode=\"hybrid\")\n",
    "\n",
    "image_query_result = mm_retriever.retrieve_image_nodes(query_bundle=QueryBundle(query_str=\"How does Llama 2 perform compared to other open-source models?\"), query_mode=\"hybrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text-dense': VectorStoreQueryResult(nodes=[TextNode(id_='bfcca5a9-46d8-40be-9060-e4da22926fee', embedding=None, metadata={'metadata': {'source_file_path': '/home/lwz/projects/llama-index/RAG_with_local_LLM/data/paper_PDF/llama2.pdf', 'elementType': 'section'}}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='A.2.2 Additional Details for Pretrained Models Evaluation\\nMMLU details. In Table 19, we report details of the MMLU (Hendrycks et al., 2020) evaluation for Llama 2 models and others open-source models.\\nStandard Benchmarks. In Table 20, we show results on several standard benchmarks.\\nCode Generation. In Table 21, we compare results of Llama 2 with popular open source models on the Human-Eval and MBPP code generation benchmarks.\\nWorld Knowledge. We evaluate the Llama 2 model together with other open-source models on the Natu-ralQuestions and TriviaQA benchmarks (Table 22).\\nReading Comprehension\\nIn Table 23 we report zero-shot and few-shot results on SQUAD and zero-shot and one-shot experiments on QUAC. Here Llama 2 performs best on all evaluation settings and models except the QUAC 0-shot where Llama 1 30B performs slightly better.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='61dab6fb-84b2-41ae-801f-3e97803b6784', embedding=None, metadata={'metadata': {'source_file_path': '/home/lwz/projects/llama-index/RAG_with_local_LLM/data/paper_PDF/llama2.pdf', 'elementType': 'section'}}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Llama 2 7B and 30B models outperform MPT models of the corresponding size on all categories besides code benchmarks. For the Falcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks. Additionally, Llama 2 70B model outperforms all open-source models.\\nIn addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al., 2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L.\\nWe also analysed the potential data contamination and share the details in Section A.6.\\nBenchmark (shots) GPT-3.5 GPT-4 PaLM PaLM-2-L Llama 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='bcdc2bc6-9b10-4502-85e5-07c9d1e14e6a', embedding=None, metadata={'metadata': {'source_file_path': '/home/lwz/projects/llama-index/RAG_with_local_LLM/data/paper_PDF/llama2.pdf', 'elementType': 'section'}}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Input\\nModels input text only.\\nOutput\\nModels generate text only.\\nModel Architecture Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\\nModel Dates\\nLlama 2 was trained between January 2023 and July 2023.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='9a823567-e4c7-462a-bbc4-c4b31f7d69d3', embedding=None, metadata={'metadata': {'source_file_path': '/home/lwz/projects/llama-index/RAG_with_local_LLM/data/paper_PDF/llama2.pdf', 'elementType': 'section'}}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Llama 2 Pretrained Model Evaluation\\nIn this section, we report the results for the Llama 1 and Llama 2 base models, MosaicML Pretrained Transformer (MPT) † † models, and Falcon (Almazrouei et al., 2023) models on standard academic benchmarks. For all the evaluations, we use our internal evaluations library. We reproduce results for the MPT and Falcon models internally. For these models, we always pick the best score between our evaluation framework and any publicly reported results.\\nIn Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety benchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The results for all the individual benchmarks are available in Section A.2.2.\\n• Code. We report the average pass@1 scores of our models on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). • Commonsense Reasoning. We report the average of PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019a), WinoGrande (Sakaguchi et al., 2021), ARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), and CommonsenseQA (Talmor et al., 2018). We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. • World Knowledge.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='46de72b0-ecc0-43ae-89cf-953709e9ec2c', embedding=None, metadata={'metadata': {'source_file_path': '/home/lwz/projects/llama-index/RAG_with_local_LLM/data/paper_PDF/llama2.pdf', 'elementType': 'title'}}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='title:\\nLlama 2: Open Foundation and Fine-Tuned Chat Models', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')], similarities=[0.751518331934734, 0.7221868981565361, 0.7207085662082724, 0.7180514136872769, 0.703473645447354], ids=['bfcca5a9-46d8-40be-9060-e4da22926fee', '61dab6fb-84b2-41ae-801f-3e97803b6784', 'bcdc2bc6-9b10-4502-85e5-07c9d1e14e6a', '9a823567-e4c7-462a-bbc4-c4b31f7d69d3', '46de72b0-ecc0-43ae-89cf-953709e9ec2c']), 'text-sparse': VectorStoreQueryResult(nodes=[TextNode(id_='bfcca5a9-46d8-40be-9060-e4da22926fee', embedding=None, metadata={'metadata': {'source_file_path': '/home/lwz/projects/llama-index/RAG_with_local_LLM/data/paper_PDF/llama2.pdf', 'elementType': 'section'}}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='A.2.2 Additional Details for Pretrained Models Evaluation\\nMMLU details. In Table 19, we report details of the MMLU (Hendrycks et al., 2020) evaluation for Llama 2 models and others open-source models.\\nStandard Benchmarks. In Table 20, we show results on several standard benchmarks.\\nCode Generation. In Table 21, we compare results of Llama 2 with popular open source models on the Human-Eval and MBPP code generation benchmarks.\\nWorld Knowledge. We evaluate the Llama 2 model together with other open-source models on the Natu-ralQuestions and TriviaQA benchmarks (Table 22).\\nReading Comprehension\\nIn Table 23 we report zero-shot and few-shot results on SQUAD and zero-shot and one-shot experiments on QUAC. Here Llama 2 performs best on all evaluation settings and models except the QUAC 0-shot where Llama 1 30B performs slightly better.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='7c2665d8-e929-4580-9231-9930c8737951', embedding=None, metadata={'metadata': {'source_file_path': '/home/lwz/projects/llama-index/RAG_with_local_LLM/data/paper_PDF/llama2.pdf', 'elementType': 'section'}}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. Human raters judged model generations for safety violations across ~2,000 adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models.\\nWe are releasing the following models to the general public for research and commercial use ‡ :\\n1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='61dab6fb-84b2-41ae-801f-3e97803b6784', embedding=None, metadata={'metadata': {'source_file_path': '/home/lwz/projects/llama-index/RAG_with_local_LLM/data/paper_PDF/llama2.pdf', 'elementType': 'section'}}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Llama 2 7B and 30B models outperform MPT models of the corresponding size on all categories besides code benchmarks. For the Falcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks. Additionally, Llama 2 70B model outperforms all open-source models.\\nIn addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al., 2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L.\\nWe also analysed the potential data contamination and share the details in Section A.6.\\nBenchmark (shots) GPT-3.5 GPT-4 PaLM PaLM-2-L Llama 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='67cf8b94-7cc4-4c00-abdc-4aff589eee73', embedding=None, metadata={'metadata': {'source_file_path': '/home/lwz/projects/llama-index/RAG_with_local_LLM/data/paper_PDF/llama2.pdf', 'elementType': 'abstract'}}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='abstract:\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='47fdb6a6-98b5-40d8-bb62-d13b7a87a624', embedding=None, metadata={'metadata': {'source_file_path': '/home/lwz/projects/llama-index/RAG_with_local_LLM/data/paper_PDF/llama2.pdf', 'elementType': 'section'}}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Conclusion\\nIn this study, we have introduced Llama 2, a new family of pretrained and fine-tuned models with scales of 7 billion to 70 billion parameters. These models have demonstrated their competitiveness with existing open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation sets we examined, although they still lag behind other models like GPT-4. We meticulously elaborated on the methods and techniques applied in achieving our models, with a heavy emphasis on their alignment with the principles of helpfulness and safety. To contribute more significantly to society and foster the pace of research, we have responsibly opened access to Llama 2 and Llama 2-Chat. As part of our ongoing commitment to transparency and safety, we plan to make further improvements to Llama 2-Chat in future work.\\nTerry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. Exploring ai ethics of chatgpt: A diagnostic analysis. arXiv preprint arXiv:2301.12867, 2023.\\n• Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original Llama team who helped get this work started. • Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the figures in the paper. • Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the internal demo. • Early reviewers of this paper, who helped us improve its quality, including Mike Lewis, Joelle Pineau, Laurens van der Maaten, Jason Weston, and Omer Levy.\\nA.2 Additional Details for Pretraining', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')], similarities=[21.382061004638672, 20.529539108276367, 19.862550735473633, 19.472213745117188, 19.237985610961914], ids=['bfcca5a9-46d8-40be-9060-e4da22926fee', '7c2665d8-e929-4580-9231-9930c8737951', '61dab6fb-84b2-41ae-801f-3e97803b6784', '67cf8b94-7cc4-4c00-abdc-4aff589eee73', '47fdb6a6-98b5-40d8-bb62-d13b7a87a624'])}\n"
     ]
    }
   ],
   "source": [
    "print(text_query_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu118py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
